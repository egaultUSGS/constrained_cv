{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably need to update these imports to the new name, PyHAT\n",
    "from point_spectra_gui.util.spectral_data import spectral_data as spectral_data\n",
    "import libpysat.regression.cv as cv2\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.linear_model import RANSACRegressor as RANSAC\n",
    "import pandas as pd\n",
    "from libpysat.regression.regression import regression\n",
    "from sklearn.linear_model import enet_path, lasso_path\n",
    "from sklearn.linear_model.base import _pre_fit\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "# Test for commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function takes the data as input, and returns a spectral data object\n",
    "def CCAM_CSV(input_data, ave=True):\n",
    "    \n",
    "    #These try/excepts are clunky but get the job done\n",
    "    try:\n",
    "        df = pd.read_csv(input_data, header=14, engine='c')\n",
    "        cols = list(df.columns.values)\n",
    "        df.columns = [i.strip().replace('# ', '') for i in cols]  # strip whitespace from column names\n",
    "        df.set_index(['wave'], inplace=True)  # use wavelengths as indices\n",
    "        # read the file header and put information into the dataframe as new columns\n",
    "        metadata = pd.read_csv(input_data, sep='=', nrows=14, comment=',', engine='c', index_col=0, header=None)\n",
    "    except:\n",
    "        try:  # handle files with an extra header row containing temperature\n",
    "            df = pd.read_csv(input_data, header=15, engine='c')\n",
    "            cols = list(df.columns.values)\n",
    "            df.columns = [i.strip().replace('# ', '') for i in cols]  # strip whitespace from column names\n",
    "            df.set_index(['wave'], inplace=True)  # use wavelengths as indices\n",
    "            # read the file header and put information into the dataframe as new columns\n",
    "            metadata = pd.read_csv(input_data, sep='=', nrows=15, comment=',', engine='c', index_col=0, header=None)\n",
    "        except:  # handle files with an extra header row containing temperature and target name\n",
    "            df = pd.read_csv(input_data, header=16, engine='c')\n",
    "            cols = list(df.columns.values)\n",
    "            df.columns = [i.strip().replace('# ', '') for i in cols]  # strip whitespace from column names\n",
    "            df.set_index(['wave'], inplace=True)  # use wavelengths as indices\n",
    "            # read the file header and put information into the dataframe as new columns\n",
    "            metadata = pd.read_csv(input_data, sep='=', nrows=16, comment=',', engine='c', index_col=0, header=None)\n",
    "\n",
    "    if ave:\n",
    "        df = pd.DataFrame(df['mean'])\n",
    "    else:\n",
    "        df = df.drop(['mean', 'median'], axis=1)\n",
    "    df.index = [['wvl'] * len(df.index),\n",
    "                df.index.values.round(4)]  # create multiindex so spectra can be easily extracted with a single key\n",
    "    df = df.T  # transpose so that each spectrum is a row\n",
    "\n",
    "    # remove extraneous stuff from the metadataindices\n",
    "    metadata.index = [i.strip().strip('# ').replace(' FLOAT', '').lower() for i in metadata.index.values]\n",
    "    metadata = metadata.T\n",
    "\n",
    "    # extract info from the file name\n",
    "    fname = os.path.basename(input_data)\n",
    "    metadata['sclock'] = fname[4:13]\n",
    "    metadata['seqid'] = fname[25:34].upper()\n",
    "    metadata['Pversion'] = fname[34:36]\n",
    "\n",
    "    # duplicate the metadata for each row in the df\n",
    "    if not ave:\n",
    "        metadata = metadata.append([metadata] * (len(df.index) - 1), ignore_index=True)\n",
    "    metadata.index = df.index  # make the indices match\n",
    "    metadata.columns = [['meta'] * len(metadata.columns), metadata.columns.values]  # make the columns into multiindex\n",
    "    df = pd.concat([metadata, df], axis=1)  # combine the spectra with the metadata\n",
    "    return df\n",
    "\n",
    "# alkali_cv.py script\n",
    "def run_cross():\n",
    "    #probably need to update these imports to the new name, PyHAT\n",
    "    best_settings = []\n",
    "    models = []\n",
    "\n",
    "    #specify the list of elements that we want to develop models for\n",
    "    elements = ['Na2O','K2O']\n",
    "\n",
    "    #set output path\n",
    "    outpath = \"/home/egault/Desktop/fork1/PySAT-master/ConstrainedData/\"\n",
    "\n",
    "    #set source data path\n",
    "    sourcepath = outpath\n",
    "\n",
    "    # set the composition ranges to consider. This allows cross validation to test out all of the different submodels\n",
    "    # that will be used in the eventual calibration\n",
    "    # yranges = [[0,100], [0,1.5], [1,6], [5,100]]\n",
    "#     yranges = [[240.1, 342.2], [380.1, 469.3], [472.0, 906.5]]\n",
    "    \n",
    "    yranges = [[0,100]]\n",
    "\n",
    "    #set the list of methods to try\n",
    "    methods = [\n",
    "                'PLS']\n",
    "    # methods = [\n",
    "    #             'PLS',\n",
    "    #             'Elastic Net',\n",
    "    #             'LASSO',\n",
    "    #             'Ridge',\n",
    "    #             'GP',\n",
    "    #             'OLS',\n",
    "    #             'BRR',\n",
    "    #             'LARS',\n",
    "    #             'OMP'\n",
    "    #             ]\n",
    "    #create a log-spaced set of alpha values to consider - this is used by a number of different methods\n",
    "    alphas = np.logspace(np.log10(0.000000001), np.log10(0.001),\n",
    "                                             num=20)\n",
    "\n",
    "    # set the parameters to consider for each method. Each parameter is in a list, so more than one value can be specified.\n",
    "    # The cross validation will evaluate every possible permutation of parameters, so beware of adding too many,\n",
    "    # especially for slower methods\n",
    "    params = {'PLS':{'n_components': [1,2,3],\n",
    "                                  'scale': [False]}}\n",
    "    # params = {'PLS':{'n_components': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    #                               'scale': [False]},\n",
    "    #             'Elastic Net':{ 'alpha': alphas,\n",
    "    #                                 'l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0],\n",
    "    #                                 'fit_intercept': [True, False],\n",
    "    #                                 'normalize': [False],\n",
    "    #                                 'precompute': [True],\n",
    "    #                                 'max_iter': [1000],\n",
    "    #                                 'copy_X': [True],\n",
    "    #                                 'tol': [0.0001],\n",
    "    #                                 'warm_start': [True],\n",
    "    #                                 'positive': [True, False],\n",
    "    #                                 'selection': ['random']},\n",
    "    #            'GP':{   'reduce_dim': ['PCA'],\n",
    "    #                     'n_components': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    #                     'regr': ['linear'],\n",
    "    #                     'corr': ['squared_exponential'],\n",
    "    #                     'storage_mode': ['light'],\n",
    "    #                     'verbose': [True],\n",
    "    #                     'theta0': [0.1],\n",
    "    #                     'normalize': [True],\n",
    "    #                     'optimizer': ['fmin_cobyla'],\n",
    "    #                     'random_start': [5]},\n",
    "    #            'OLS':{  'fit_intercept': [True, False]},\n",
    "    #            'LASSO':{'alpha': list(alphas),\n",
    "    #                     'fit_intercept': [True, False],\n",
    "    #                     'max_iter': [1000],\n",
    "    #                     'tol': [0.0001],\n",
    "    #                     'positive': [True, False],\n",
    "    #                     'selection': ['random']},\n",
    "    #            'BRR':{  'n_iter': [300],\n",
    "    #                     'tol': [0.001],\n",
    "    #                     'alpha_1': [0.000001],\n",
    "    #                     'alpha_2': [0.000001],\n",
    "    #                     'lambda_1': [0.000001],\n",
    "    #                     'lambda_2': [0.000001],\n",
    "    #                     'compute_score': [False],\n",
    "    #                     'fit_intercept': [True, False],\n",
    "    #                     'normalize': [False],\n",
    "    #                     'copy_X': [True],\n",
    "    #                     'verbose': [True]},\n",
    "    #\n",
    "    #            'LARS':{ 'fit_intercept': [True, False],\n",
    "    #                     'verbose': [True],\n",
    "    #                     'normalize': [False],\n",
    "    #                     'precompute': ['auto'],\n",
    "    #                     'n_nonzero_coefs': [1,5,10,50,100,200,400,800],\n",
    "    #                     'copy_X': [True],\n",
    "    #                     'fit_path': [False],\n",
    "    #                     'positive': [True, False]},\n",
    "    #            'OMP':{'n_nonzero_coefs': [1,5,10,50,100,200,400,800],\n",
    "    #                   'fit_intercept': [True, False],\n",
    "    #                   'normalize': [False],\n",
    "    #                   'precompute': ['auto'],\n",
    "    #                   },\n",
    "    #            'Ridge':{'alpha': list(alphas),\n",
    "    #                     'copy_X': [True],\n",
    "    #                     'fit_intercept': [True, False],\n",
    "    #                     'max_iter': [None],\n",
    "    #                     'normalize': [False],\n",
    "    #                     'solver': ['auto'],\n",
    "    #                     'tol': [0.001],\n",
    "    #                     'random_state': [None]}\n",
    "    #             }\n",
    "    \n",
    "    # Filepath to the maskfile you would like to use\n",
    "    maskfile = \"/home/egault/Desktop/fork1/PySAT-master/mask_minors_noise_20160202_adjusted.csv\"\n",
    "    \n",
    "    # Empty List to hold Cross Validation prediction keys produced by do_cv().\n",
    "    cvpredictkeys = []\n",
    "    \n",
    "    # Use the read_excel() method if dealing with .xlsx reference target files.\n",
    "    # This may require an additional import.\n",
    "\n",
    "    # Obtain reference target file, then pack it into a dataframe.\n",
    "    mars_reftar_file = \"/home/egault/Desktop/fork1/PySAT-master/mars_reference_targets_simplified.csv\"\n",
    "    mars_reftar_df = pd.read_csv(mars_reftar_file)\n",
    "    \n",
    "    \n",
    "    #Step through each of the elements listed\n",
    "    for element in elements:\n",
    "        #get the data normalized to 3 (each spectrometer separately) or to 1 (sum of all spectrometers)\n",
    "        filenames = [sourcepath+\"data1_\"+element+\"_train.csv\",sourcepath+\"data3_\"+element+\"_train.csv\"]\n",
    "        print(\"Filenames: \", filenames)\n",
    "        #Step through each data file\n",
    "        for file in filenames:\n",
    "            # Read the data in, make it a \"spectral_data\" object\n",
    "            data = spectral_data(pd.read_csv(file, header=[0, 1], verbose=True))\n",
    "            #Step through each regression method\n",
    "            for method in methods:\n",
    "                paramstemp = list(ParameterGrid(params[method])) #get the parameters for this method\n",
    "                cv_obj = cv2.cv(paramstemp) #set up cross validation across all permutations of parameters\n",
    "\n",
    "                #do the cross validation for each composition range\n",
    "                for yrange in yranges:\n",
    "                    #set up an output file name that specifies the current composition range being considered\n",
    "                    if file == filenames[0]:\n",
    "                        outfile_root = 'data1_' + element + '_' + str(yrange[0])+'-'+str(yrange[1])+'_'\n",
    "                    if file == filenames[1]:\n",
    "                        outfile_root = 'data3_' + element + '_' + str(yrange[0])+'-'+str(yrange[1])+'_'\n",
    "\n",
    "                    #apply yrange to filter the compositions used in the regression\n",
    "                    y = np.array(data.df[('comp',element)])\n",
    "                    match = np.squeeze((y > yrange[0]) & (y < yrange[1]))\n",
    "                    datatemp = spectral_data(data.df.ix[match])\n",
    "                    datatemp.mask(maskfile, 'wvl')\n",
    "                    \n",
    "                    # Try to create a cv_iterator object, as it is required to run the do_cv method.\n",
    "                    try:\n",
    "                        cv_iterator = LeaveOneGroupOut().split(datatemp.df['wvl'], datatemp.df[('comp', element)], datatemp.df[\n",
    "                            ('meta', 'Folds')])  # create an iterator for cross validation based on the predefined folds\n",
    "                        n_folds = LeaveOneGroupOut().get_n_splits(groups=datatemp.df[('meta', 'Folds')])\n",
    "\n",
    "                    except:\n",
    "                        print('***No folds found! Did you remember to define folds before running cross validation?***')\n",
    "                    \n",
    "                    #do the actual cross validation for the current method, element, and yrange\n",
    "                    datatemp.df, cv_results, cvmodels, cvmodelkeys, cvpredictkeys = cv_obj.do_cv(datatemp.df, cv_iterator,\n",
    "                                                                                  xcols = 'wvl',\n",
    "                                                                                  ycol = ('comp', 'Na2O'), method=method,\n",
    "                                                                                  yrange=yrange, calc_path=False,\n",
    "                                                                                  n_folds = n_folds)\n",
    "\n",
    "                    \n",
    "                    # Filepath to read in training data\n",
    "                    training_data_filepath = \"/home/egault/Desktop/fork1/PySAT-master/FullData/\"\n",
    "                    \n",
    "                    # Filepath to read in mars data\n",
    "                    mars_filepath = \"/home/egault/Desktop/fork1/PySAT-master/reference_data\"\n",
    "                    \n",
    "                    # This grabs all .csv files and puts them in a list\n",
    "                    allFiles = glob.glob(mars_filepath + \"/*.csv\")\n",
    "                    # Iterates the entire list\n",
    "                    for file_ in allFiles:\n",
    "                            filename = os.path.splitext(file_)\n",
    "                            filename = os.path.basename(filename[0])\n",
    "                            \n",
    "                            # Converts the .csv to readable format for the spectral data object\n",
    "                            mars_df = CCAM_CSV(file_)\n",
    "                            \n",
    "                            # Converts the .csv to a spectral data object\n",
    "                            mars_df_obj = spectral_data(mars_df)\n",
    "                            \n",
    "                            # Iterates over the modesls that were returned \n",
    "                            for model in cvmodels:\n",
    "                                \n",
    "                                # Normalization causes following error with certain mars files:\n",
    "                                # ValueError: cannot handle a non-unique multi-index!\n",
    "                                \n",
    "                                # A stable file that is within the current reference target spreadsheet\n",
    "                                # is named:CL5_433631305CCS_F0170000CCAM01407P1.csv\n",
    "                                # This file was ideal for testing purposes.\n",
    "                                \n",
    "                                mars_df_obj.norm([[0, 1000]], 'wvl')\n",
    "                                \n",
    "                                \n",
    "                                # Masks values based on the maskfile you passed\n",
    "                                mars_df_obj.mask(maskfile, 'wvl')\n",
    "                                \n",
    "                                # Runs the model and predicts a value for the wavelengths \n",
    "                                predict = model.predict(mars_df_obj.df['wvl'])\n",
    "                                \n",
    "                                # Obtain minimum value for prediction check.\n",
    "                                \n",
    "                                # Get label of min value column.\n",
    "                                min_col_key = element + ' (min)'\n",
    "                                \n",
    "                                # Prints the values\n",
    "                                print(\"Predicted: \", predict)\n",
    "    \n",
    "                    # save cross validation results\n",
    "                    filename = outpath + outfile_root + method + '_CV.csv'\n",
    "                    cv_results['cv'].to_csv(filename)\n",
    "\n",
    "                    # save cross validation predictions\n",
    "                    filename = outpath + outfile_root + method + '_CV_results.csv'\n",
    "                    temp = datatemp.df.drop('wvl', axis=1, level=0)                   \n",
    "                    \n",
    "                    \n",
    "                    temp.to_csv(filename)\n",
    "\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filenames:  ['/home/egault/Desktop/fork1/PySAT-master/ConstrainedData/data1_Na2O_train.csv', '/home/egault/Desktop/fork1/PySAT-master/ConstrainedData/data3_Na2O_train.csv']\n",
      "Tokenization took: 67.30 ms\n",
      "Type conversion took: 137.32 ms\n",
      "Parser memory cleanup took: 0.20 ms\n",
      "Tokenization took: 62.50 ms\n",
      "Type conversion took: 134.28 ms\n",
      "Parser memory cleanup took: 0.14 ms\n",
      "Tokenization took: 63.74 ms\n",
      "Type conversion took: 133.23 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.64 ms\n",
      "Type conversion took: 139.22 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 61.45 ms\n",
      "Type conversion took: 138.01 ms\n",
      "Parser memory cleanup took: 0.13 ms\n",
      "Tokenization took: 64.32 ms\n",
      "Type conversion took: 139.44 ms\n",
      "Parser memory cleanup took: 0.13 ms\n",
      "Tokenization took: 62.34 ms\n",
      "Type conversion took: 143.51 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 61.75 ms\n",
      "Type conversion took: 140.14 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 61.94 ms\n",
      "Type conversion took: 144.02 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 70.33 ms\n",
      "Type conversion took: 143.29 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 62.51 ms\n",
      "Type conversion took: 146.81 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 62.47 ms\n",
      "Type conversion took: 144.98 ms\n",
      "Parser memory cleanup took: 0.14 ms\n",
      "Tokenization took: 71.08 ms\n",
      "Type conversion took: 157.48 ms\n",
      "Parser memory cleanup took: 0.13 ms\n",
      "Tokenization took: 65.79 ms\n",
      "Type conversion took: 174.90 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 71.46 ms\n",
      "Type conversion took: 148.99 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 12.62 ms\n",
      "Type conversion took: 91.64 ms\n",
      "Parser memory cleanup took: 0.43 ms\n",
      "{'n_components': 1, 'scale': False}\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "1888\n",
      "{'n_components': 2, 'scale': False}\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "1888\n",
      "{'n_components': 3, 'scale': False}\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "1888\n",
      "1\n",
      "Predicted:  [[4.9181]]\n",
      "1\n",
      "Predicted:  [[11.1618]]\n",
      "1\n",
      "Predicted:  [[2.2132]]\n",
      "Tokenization took: 66.80 ms\n",
      "Type conversion took: 141.21 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.86 ms\n",
      "Type conversion took: 140.07 ms\n",
      "Parser memory cleanup took: 0.13 ms\n",
      "Tokenization took: 60.58 ms\n",
      "Type conversion took: 142.30 ms\n",
      "Parser memory cleanup took: 0.14 ms\n",
      "Tokenization took: 62.04 ms\n",
      "Type conversion took: 144.09 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.92 ms\n",
      "Type conversion took: 140.57 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.04 ms\n",
      "Type conversion took: 143.37 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.55 ms\n",
      "Type conversion took: 135.03 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 64.37 ms\n",
      "Type conversion took: 135.76 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.34 ms\n",
      "Type conversion took: 138.75 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.02 ms\n",
      "Type conversion took: 137.77 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 63.54 ms\n",
      "Type conversion took: 135.06 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.04 ms\n",
      "Type conversion took: 142.45 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 61.36 ms\n",
      "Type conversion took: 136.99 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 66.14 ms\n",
      "Type conversion took: 141.94 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 63.59 ms\n",
      "Type conversion took: 138.88 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 11.17 ms\n",
      "Type conversion took: 80.88 ms\n",
      "Parser memory cleanup took: 0.47 ms\n",
      "{'n_components': 1, 'scale': False}\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "1888\n",
      "{'n_components': 2, 'scale': False}\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "1888\n",
      "{'n_components': 3, 'scale': False}\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "495\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "475\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "485\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "433\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "1888\n",
      "1\n",
      "Predicted:  [[4.0577]]\n",
      "1\n",
      "Predicted:  [[1.9658]]\n",
      "1\n",
      "Predicted:  [[3.078]]\n",
      "Filenames:  ['/home/egault/Desktop/fork1/PySAT-master/ConstrainedData/data1_K2O_train.csv', '/home/egault/Desktop/fork1/PySAT-master/ConstrainedData/data3_K2O_train.csv']\n",
      "Tokenization took: 64.85 ms\n",
      "Type conversion took: 138.91 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.89 ms\n",
      "Type conversion took: 132.68 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 64.49 ms\n",
      "Type conversion took: 132.42 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.83 ms\n",
      "Type conversion took: 138.49 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.51 ms\n",
      "Type conversion took: 133.45 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 64.66 ms\n",
      "Type conversion took: 133.75 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.66 ms\n",
      "Type conversion took: 139.52 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.25 ms\n",
      "Type conversion took: 132.67 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 64.91 ms\n",
      "Type conversion took: 133.03 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.71 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type conversion took: 139.75 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.44 ms\n",
      "Type conversion took: 132.60 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 62.94 ms\n",
      "Type conversion took: 134.26 ms\n",
      "Parser memory cleanup took: 0.14 ms\n",
      "Tokenization took: 61.55 ms\n",
      "Type conversion took: 139.11 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 62.40 ms\n",
      "Type conversion took: 134.45 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 63.97 ms\n",
      "Type conversion took: 134.94 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 8.30 ms\n",
      "Type conversion took: 74.58 ms\n",
      "Parser memory cleanup took: 0.35 ms\n",
      "{'n_components': 1, 'scale': False}\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "1847\n",
      "{'n_components': 2, 'scale': False}\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "1847\n",
      "{'n_components': 3, 'scale': False}\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "1847\n",
      "1\n",
      "Predicted:  [[6.1944]]\n",
      "1\n",
      "Predicted:  [[0.1163]]\n",
      "1\n",
      "Predicted:  [[2.0772]]\n",
      "Tokenization took: 64.61 ms\n",
      "Type conversion took: 137.30 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.75 ms\n",
      "Type conversion took: 133.88 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 61.74 ms\n",
      "Type conversion took: 132.95 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.30 ms\n",
      "Type conversion took: 137.62 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.12 ms\n",
      "Type conversion took: 134.41 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 61.96 ms\n",
      "Type conversion took: 135.67 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.27 ms\n",
      "Type conversion took: 136.76 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 59.76 ms\n",
      "Type conversion took: 134.02 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 61.63 ms\n",
      "Type conversion took: 133.65 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.31 ms\n",
      "Type conversion took: 136.12 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 60.12 ms\n",
      "Type conversion took: 134.54 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 62.96 ms\n",
      "Type conversion took: 134.25 ms\n",
      "Parser memory cleanup took: 0.14 ms\n",
      "Tokenization took: 60.25 ms\n",
      "Type conversion took: 137.13 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 61.93 ms\n",
      "Type conversion took: 135.29 ms\n",
      "Parser memory cleanup took: 0.11 ms\n",
      "Tokenization took: 63.95 ms\n",
      "Type conversion took: 146.64 ms\n",
      "Parser memory cleanup took: 0.12 ms\n",
      "Tokenization took: 8.24 ms\n",
      "Type conversion took: 76.20 ms\n",
      "Parser memory cleanup took: 0.13 ms\n",
      "{'n_components': 1, 'scale': False}\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 1, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=1, scale=False, tol=1e-06)\n",
      "1847\n",
      "{'n_components': 2, 'scale': False}\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 2, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\n",
      "1847\n",
      "{'n_components': 3, 'scale': False}\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "440\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "434\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "465\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "508\n",
      "[{'n_components': 3, 'scale': False}]\n",
      "PLSRegression(copy=True, max_iter=500, n_components=3, scale=False, tol=1e-06)\n",
      "1847\n",
      "1\n",
      "Predicted:  [[2.9992]]\n",
      "1\n",
      "Predicted:  [[1.5792]]\n",
      "1\n",
      "Predicted:  [[1.1184]]\n"
     ]
    }
   ],
   "source": [
    "run_cross()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ef797c2b3b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
